---
title: "Data_prep"
author: "RaphaÃ«l Morsomme"
date: "January 25, 2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE, warning=FALSE}
#knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(tidytext) # sentiment analysis
library(corrplot) # Q3 figure


# Read in data
d <- read_csv("AB_NYC_2019.csv",
              col_types =  cols(neighbourhood_group = col_factor(),
                                neighbourhood = col_factor(),
                                room_type = col_factor()))
d_attraction <- read_csv("subway and attractions data/attrac_loc.csv")
d_metro      <- read_csv("subway and attractions data/subway_loc.csv")

# Sentiment analysis
dictionnary <- get_sentiments("afinn") # ranges between -5 and 5.
compute_sentiment <- function(x){
  tibble(term = x) %>%
    unnest_tokens(word, term) %>%
    left_join(dictionnary, by = "word") %>% replace_na(list(score = 0)) %>%
    pull(score) %>% mean }

# Distance to closest metro
compute_distance_closest_metro <- function(longitude, latitude){
  dist <- abs(longitude - d_metro$Station.Longitude) + abs(latitude - d_metro$Station.Latitude) # manhattan distance (instead of euclidean distance)
  min(dist) }

# Distance to attraction
compute_proximity_attractions <- function(longitude, latitude){
  d_attraction %>% 
    rename(long = longitude, lat = latitude) %>% # prevents use of same names
    mutate(distance  = abs(long - longitude) + abs(lat - latitude),
           proximity = 1 / (distance + 0.01)) %>%
#    top_n(5, proximity) %>%
    summarize(proximity = mean(proximity)) %>%
    pull(proximity) }


# Name frquency
freq <- d %>% distinct(host_id, host_name) %>% pull(host_name) %>% table # only count each id once (do not count people with multiple listing muliple times)
name_freq <- tibble(host_name = names(freq), name_host_freq = as.vector(freq) / sum(freq)) %>% arrange(-name_host_freq)
```

```{r data prep sentiment, cache = TRUE}
d <- d %>%
  
  # takes a while so I cached these
  
  # Sentiment of listing name
  mutate(name_listing_sentiment = name %>% map_dbl(compute_sentiment)) %>%
  
  # Proximity to attractions
  mutate(proximity_attraction = list(longitude, latitude) %>% pmap_dbl(compute_proximity_attractions))
```

```{r data prep}
d_clean <- d %>%
  
  
  ## Cleaning
  
  # Active listings
  mutate(days_since_last = max(last_review, na.rm = TRUE) - last_review) %>%
  filter(!is.na(last_review))  %>% # removes 10,000 (effectively removes people with no review)
  filter(days_since_last <= 365) %>% # removes 9,000
  
  ## Feature engineering
  
  # Y1: Popularity
  mutate(popularity = if_else(availability_365 > 10, reviews_per_month / availability_365, NA_real_)) %>%
  mutate(popularity_log = log(popularity + 1)) %>% 
  
  # Y2: price
  mutate(price_log = log(price + 1)) %>%
  
  # Host name
  left_join(name_freq, by = "host_name") %>% # frequency
  mutate(name_host_special = grepl('[^[:alpha:]]', host_name)) %>% # check if name contains a special character (not a letter)
  
  # Listing name
  mutate(name_listing_length = nchar(name)) %>% # number of characters
  # sentiment computed in previous chunk

  # Proximity to closest metro station
  mutate(dist_closest_metro = list(longitude, latitude) %>% pmap_dbl(compute_distance_closest_metro),
         proximity_metro = 1 / (dist_closest_metro + 0.01) )
```

```{r}
save(d_clean, file = "d_clean.RDATA")
```

```{r}
d_clean_inter <- d_clean %>%
  filter(minimum_nights < 29) %>% 
  filter(calculated_host_listings_count <= 5) %>%
  filter(!is.na(name_listing_length)) %>%
  filter(!is.na(name_host_freq)) %>%
  select(-id,-name,-host_id, -host_name, -price, -neighbourhood, -last_review,
         -days_since_last,-availability_365, -dist_closest_metro, -popularity) %>% 
  rename(listing_count = calculated_host_listings_count) %>% 
  rename(listing_sentiment= name_listing_sentiment)


d_clean_price_rf <- d_clean_inter %>% 
  select(-popularity_log) %>% 
  select(price_log, everything()) %>% 
  filter(!is.na(price_log))

d_clean_pop_rf <- d_clean_inter %>% 
  select(-price_log) %>% 
  select(popularity_log, everything()) %>% 
  filter(!is.na(popularity_log))

d_clean_price_bma<- d_clean_inter %>% 
  select(-popularity_log, -longitude, -latitude) %>% 
  select(price_log, everything()) %>% 
  filter(!is.na(price_log))

d_clean_pop_bma <- d_clean_inter %>% 
  select(-price_log, -longitude, -latitude) %>%
  select(popularity_log, everything()) %>% 
  filter(!is.na(popularity_log))
```


```{r}
save(d_clean_price_rf, d_clean_pop_rf, file = "d_clean_rf.RDATA")
save(d_clean_price_bma, d_clean_pop_bma, file = "d_clean_bma.RDATA")
```

```{r datasets no outliers}
price_sumstats <- d_clean_price_rf%>%
  summarise(avg_price = mean(price_log),
            sd_price = sd(price_log),
            min_price = min(price_log), 
            max_price = max(price_log))

d_clean_price_rf_no_outliers <- d_clean_price_rf %>% 
  filter(near(price_log, price_sumstats$avg_price, tol = 3*price_sumstats$sd_price))

d_clean_price_bma_no_outliers <- d_clean_price_bma %>% 
  filter(near(price_log, price_sumstats$avg_price, tol = 3*price_sumstats$sd_price))

pop_sumstats <- d_clean_pop_rf %>% 
  summarise(avg_pop = mean(popularity_log), 
            sd_pop = sd(popularity_log),
            min_pop = min(popularity_log),
            max_pop = max(popularity_log))

d_clean_pop_rf_no_outliers <- d_clean_pop_rf %>% 
  filter(near(popularity_log, pop_sumstats$avg_pop, tol = 3*pop_sumstats$sd_pop)) 

d_clean_pop_bma_no_outliers <- d_clean_pop_bma %>% 
  filter(near(popularity_log, pop_sumstats$avg_pop, tol = 3*pop_sumstats$sd_pop)) 

```

```{r}
save(d_clean_price_rf_no_outliers,d_clean_pop_rf_no_outliers, file = "d_clean_rf_no_outliers.RDATA")
save(d_clean_pop_bma_no_outliers,d_clean_price_bma_no_outliers, file = "d_clean_bma_no_outliers.RDATA")
```


